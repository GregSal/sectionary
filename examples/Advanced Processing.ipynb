{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced Processing\n",
    "In this tutorial we introduce some more advanced tools for section processing.\n",
    "\n",
    "1. Processing using a generator function\n",
    "2. Rules and Rule Sets\n",
    "3. Regular expression for processing text\n",
    "4. The FixedWidth and csv Parsing tools\n",
    "5. Making use of the Context dictionary\n",
    "    1. Setting context values when calling Section.read\n",
    "    2. Accessing default context values\n",
    "6. Passing other parameters to the processing methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing with a generator function\n",
    "\n",
    "In a simple processor function the input is a single item from the section and\n",
    "the output is one \"item\" for each \"input item\".\n",
    "\n",
    "The relation between individual input (section) items and the resulting \n",
    "processed items is not necessarily 1:1. A single input item could be broken up \n",
    "into multiple processed items, or conversely, multiple section items could be \n",
    "converted into one processed item:\n",
    "\n",
    "    \t  • 1 SourceItem → 1 ProcessedItem\n",
    "    \t  • 1 SourceItem → 2+ ProcessedItems\n",
    "    \t  • 2+ SourceItems → 1 ProcessedItem\n",
    "\n",
    "In general, regular functions are only used when there is a one-to-one \n",
    "correspondence between input item and output item.  Generator functions are used \n",
    "when multiple input items are required to generate a processed item, or when one \n",
    "input item results in multiple processed items.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rule and RuleSets\n",
    "Instead of having one function `process_directory()` that manages all possible \n",
    "text lines in the section, the function can be broken down into parts by \n",
    "defining *Rules*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rules\n",
    "Rules define an action to take on an item depending on the result of a test.\n",
    "\n",
    "A *Rule* definition has two parts:\n",
    "1. Trigger:\n",
    "   > Defines the test to be applied to the source item\n",
    "   > Trigger related arguments:\n",
    "   > - sentinel\n",
    "   >   - For string items, sentinel can be a string or compiled regular expression.\n",
    "   > - location\n",
    "   >   - A sentinel modifier that applies to str or re.Pattern types of sentinels. One of  ['IN', 'START', 'END', 'FULL', None]. Default is None, which is treated as 'IN'\n",
    "\n",
    "2. Action\n",
    "   > Defines the actions to take depending on the Trigger outcome.\n",
    "   > Action related arguments:\n",
    "   > - pass_method\n",
    "   > - fail_method\n",
    "   >\n",
    "   > Both take functions, or the name of standard actions to be implemented if the test passes or fails respectively.\n",
    "   >\n",
    "   > The pass_method and fail_method functions can be simple process functions, with one positional argument and additional keyword arguments. The functions can also contain a second positional argument *event* which allows the function to access information about the test results.  This is particularly useful when the sentinel is a regular expression.\n",
    "   >\n",
    "   > pass_method and fail_method can also be a string with the name of one of the standard actions.  The most common are:\n",
    "   > - 'Original': return the item being.\n",
    "   > - 'None': return None\n",
    "   > - 'Blank': return ''  (an empty string)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RuleSets\n",
    "RuleSets combine related Rules to provide multiple choices for actions.\n",
    "RuleSets\n",
    "    are used when the function that should be applied to the SourceItem(s)\n",
    "    depends on the result of one or more tests (Triggers).  Individual Rules can\n",
    "    be used when only a single Trigger is required (by using both the Pass and\n",
    "    Fail methods of the Rule) or to modify some of the SourceItems while leaving\n",
    "    others unchanged (by setting the Fail method to 'Original').  For Rules or\n",
    "    RuleSets it is important that the output is of the same type regardless of\n",
    "    whether the Trigger(s) pass or fail.\n",
    "\n",
    "- A Rule Set takes A sequence of Rules and a default method.\n",
    "- Each Rule in the sequence will be applied to the input until One of the rules triggers. At that point The sequence ends.  \n",
    "- If no Rule triggers then the default method is applied.  \n",
    "- Each of the Rules (and the default) should expect the same input type and should produce the same output type.  \n",
    "- The default_method can be any valid process function or standard action.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "*Triggers*, *TriggerEvent*, *Rules* and *RuleSets* will be covered in more detail in a separate tutorial.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert the Process Directory Function into Rules\n",
    "The process_directory function consists of a set of `if` statements which each call a different function.  Each `if` statement can be converted into is own rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get the directory name\n",
    "```\n",
    "if 'Directory of' in dir_line:\n",
    "    output_line = dir_name_split(dir_line)\n",
    "```\n",
    "**Becomes the Rule:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_name_rule = Rule('Directory of', pass_method=dir_name_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label the subdirectories\n",
    "```\n",
    "elif '<DIR>' in dir_line:\n",
    "    output_line = get_subfolder_name(dir_line)\n",
    "```\n",
    "**Becomes the Rule:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subfolder_rule = Rule('<DIR>', pass_method=get_subfolder_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label the file counts\n",
    "```\n",
    "elif 'File(s)' in dir_line:\n",
    "    output_line = file_count_split(dir_line)\n",
    "```\n",
    "**Becomes the Rule:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_count_rule = Rule('File(s)', pass_method=file_count_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Label the files\n",
    "```\n",
    "else:\n",
    "    output_line = get_file_name(dir_line)\n",
    "\n",
    "```\n",
    "This is not converted into a rule because there is no conditional.  Instead it becaomes the default method for a *RuleSet*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_process = RuleSet([dir_name_rule, subfolder_rule, file_count_rule], \n",
    "                      default=get_file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Dir Section Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder Name:\tTest Dir Structure\n",
      "\tFile:\t\t\n",
      "\tSubdirectory:\t   .\n",
      "\tSubdirectory:\t   ..\n",
      "\tSubdirectory:\t   Dir1\n",
      "\tSubdirectory:\t   Dir2\n",
      "\tFile:\t\t 3 TestFile1.txt\n",
      "\tFile:\t\t 7 TestFile2.rtf\n",
      "\tFile:\t\t 0 TestFile3.docx\n",
      "\tFile:\t\t91 xcopy.txt\n",
      "Number of Files:\t4\n"
     ]
    }
   ],
   "source": [
    "dir_section = Section(start_section='Directory of', \n",
    "                      end_section=SectionBreak('File(s)', break_offset='After'),\n",
    "                      processor=[dir_process])\n",
    "\n",
    "output = dir_section.read(dir_text)\n",
    "for line in output:\n",
    "    print(line)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regex based processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#%% Regex Parsing patterns\n",
    "# File Count and summary:\n",
    "     #          1 File(s)          59904 bytes\n",
    "     #         23 Dir(s)     63927545856 bytes free\n",
    "folder_summary_pt = re.compile(\n",
    "    '(?P<files>'       # beginning of files string group\n",
    "    '[0-9]+'           # Integer number of files\n",
    "    ')'                # end of files string group\n",
    "    '[ ]+'             # Arbitrary number of spaces\n",
    "    '(?P<type>'        # beginning of type string group\n",
    "    'File|Dir'         # \"File\" or \" Dir\" text\n",
    "    ')'                # end of type string group\n",
    "    '\\\\(s\\\\)'          # \"(s)\" text\n",
    "    '[ ]+'             # Arbitrary number of spaces\n",
    "    '(?P<size>'        # beginning of size string group\n",
    "    '[0-9]+'           # Integer size of folder\n",
    "    ')'                # end of size string group\n",
    "    ' bytes'           # \"bytes\" text\n",
    "    )\n",
    "date_pattern = tp.build_date_re(compile_re=False)\n",
    "file_listing_pt = re.compile(\n",
    "    f'{date_pattern}'  # Insert date pattern\n",
    "    '[ ]+'             # Arbitrary number of spaces\n",
    "    '(?P<size>'        # beginning of size string group\n",
    "    '[0-9]+'           # Integer size of folder\n",
    "    ')'                # end of size string group\n",
    "    ' '                # Single space\n",
    "    '(?P<filename>'    # beginning of filename string group\n",
    "    '.*'               # Integer size of folder\n",
    "    ')'                # end of size string group\n",
    "    '$'                # end of string\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#%% Line Parsing Functions\n",
    "# Directory Label Rule\n",
    "\n",
    "def extract_directory(line: str, event, *args,\n",
    "                    context=None, **kwargs) -> List[List[str]]:\n",
    "    '''Extract Directory path from folder header.\n",
    "    '''\n",
    "    full_dir = line.replace('Directory of', '').strip()\n",
    "    return [full_dir]\n",
    "\n",
    "\n",
    "dir_header_rule = Rule(\n",
    "    name='Dir Header Rule',\n",
    "    sentinel='Directory of ',\n",
    "    pass_method=extract_directory\n",
    "    )\n",
    "\n",
    "\n",
    "# skip <DIR>\n",
    "def blank_line(*args, **kwargs) -> List[List[str]]:\n",
    "    return [['']]\n",
    "\n",
    "\n",
    "skip_dir_rule = Rule(\n",
    "    name='Skip <DIR> Rule',\n",
    "    sentinel=' <DIR> ',\n",
    "    pass_method='Blank'\n",
    "    )\n",
    "skip_totals_rule = Rule(\n",
    "    name='Skip Total Files Header Rule',\n",
    "    sentinel='Total Files Listed:',\n",
    "    pass_method='Blank'\n",
    "    )\n",
    "\n",
    "\n",
    "# Regular file listings\n",
    "def file_parse(line: str, event, *args, **kwargs) -> List[List[str]]:\n",
    "    '''Break file data into three columns containing Filename, Date, Size.\n",
    "\n",
    "    Typical file is:\n",
    "        2016-02-25  22:59     3 TestFile1.txt\n",
    "    File line is parsed using a regular expression with 3 named groups.\n",
    "    Output for the example above is:\n",
    "        [[TestFile1.txt , 2016-02-25  22:59, 3]]\n",
    "\n",
    "    Args:\n",
    "        line (str): The text line to be parsed.\n",
    "        event (re.match): The results of the trigger test on the line.\n",
    "            Contains 3 named groups: ['date', 'size', 'filename'].\n",
    "        *args & **kwargs: Catch unused extra parameters passed to file_parse.\n",
    "\n",
    "    Returns:\n",
    "        tp.ParseResults: A one-item list containing the parsed file\n",
    "            information as a 3-item tuple:\n",
    "                [(filename: str, date: str, file size: int)].\n",
    "    '''\n",
    "    file_line_parts = event.test_value.groupdict(default='')\n",
    "    parsed_line = tuple([\n",
    "        file_line_parts['filename'],\n",
    "        tp.make_date_time_string(event),\n",
    "        int(file_line_parts['size'])\n",
    "        ])\n",
    "    return parsed_line\n",
    "\n",
    "\n",
    "# Regular File Parsing Rule\n",
    "file_listing_rule = Rule(file_listing_pt, pass_method=file_parse,\n",
    "                            name='Files_rule')\n",
    "\n",
    "\n",
    "# File Count Parsing Rule\n",
    "def file_count_parse(line: str, event, *args, **kwargs) -> List[List[str]]:\n",
    "    '''Break file data into two rows containing:\n",
    "           Number of files, & Directory size.\n",
    "\n",
    "    Output has the following format:\n",
    "        ['Number of files', file count value: int]\n",
    "        ['Directory Size', directory size value: int]\n",
    "\n",
    "    Typical line is:\n",
    "        4 File(s)           3501 bytes\n",
    "    File count is parsed using a regular expression with 2 named groups.\n",
    "\n",
    "    Args:\n",
    "        line (str): The text line to be parsed.\n",
    "        event (re.match): The results of the trigger test size the line.\n",
    "            Contains 3 named groups: ['files', 'type', 'size'].\n",
    "        *args & **kwargs: Catch unused extra parameters passed to file_parse.\n",
    "\n",
    "    Returns:\n",
    "        tp.ParseResults: The parsed file information.\n",
    "            The parsed file information consists of three lines with the\n",
    "            following format:\n",
    "                'Number of files', file count value: int\n",
    "                'Directory Size', directory size value: int\n",
    "    '''\n",
    "    file_count_parts = event.groupdict(default='')\n",
    "    # Manage case where bytes free is given:\n",
    "    # 23 Dir(s)     63927545856 bytes free\n",
    "    if line.strip().endswith('free'):\n",
    "        file_count_parts['size_label'] = 'Free Space'\n",
    "    else:\n",
    "        file_count_parts['size_label'] = 'Size'\n",
    "    parsed_line_template = ''.join([\n",
    "        'Number of {type}s, {files}\\n',\n",
    "        'Directory {size_label}, {size}'\n",
    "        ])\n",
    "    parsed_line_str = parsed_line_template.format(**file_count_parts)\n",
    "    parsed_line = [new_line.split(',')\n",
    "                   for new_line in parsed_line_str.splitlines()]\n",
    "    return parsed_line\n",
    "file_count_rule = Rule(folder_summary_pt, pass_method=file_count_parse,\n",
    "                          name='Files_rule')\n",
    "\n",
    "\n",
    "skip_file_count_rule = Rule(\n",
    "    name='Skip File(s) Rule',\n",
    "    sentinel=folder_summary_pt,\n",
    "    pass_method='Blank'\n",
    "    )\n",
    "\n",
    "\n",
    "# Files / DIRs Parse\n",
    "def make_files_rule() -> Rule:\n",
    "    '''If  File(s) or  Dir(s) extract # files & size\n",
    "        '''\n",
    "    def files_total_parse(line, event, *args, **kwargs) -> List[List[str]]:\n",
    "        '''Break file counts into three columns containing:\n",
    "           Type (File or Dir), Count, Size.\n",
    "\n",
    "        The line:\n",
    "               11 File(s)          72507 bytes\n",
    "        Results in:\n",
    "            [('File', 11, 3501)]\n",
    "        The line:\n",
    "           23 Dir(s)     63927545856 bytes free\n",
    "        Results in:\n",
    "            [('Dir', 23, 3501)]\n",
    "\n",
    "    Args:\n",
    "        line (str): The text line to be parsed.\n",
    "        event (re.match): The results of the trigger test on the line.\n",
    "            Contains 3 named groups: ['type', 'files', 'size'].\n",
    "        *args & **kwargs: Catch unused extra parameters passed to file_parse.\n",
    "\n",
    "    Returns:\n",
    "        tp.ParseResults: A one-item list containing the parsed file count\n",
    "            information as a 3-item tuple:\n",
    "                [(Type: str (File or Dir), Count: int, Size: int)].\n",
    "        '''\n",
    "        files_dict = event.test_value.groupdict(default='')\n",
    "        parsed_line = tuple([\n",
    "            files_dict[\"type\"],\n",
    "            files_dict[\"files\"],\n",
    "            files_dict[\"size\"]\n",
    "            ])\n",
    "        return [parsed_line]\n",
    "\n",
    "    files_total_rule = Rule(folder_summary_pt,\n",
    "                               pass_method=files_total_parse,\n",
    "                               name='Files_Total_rule')\n",
    "    return files_total_rule\n",
    "\n",
    "\n",
    "default_csv = tp.define_csv_parser('dir_files', delimiter=':',\n",
    "                                       skipinitialspace=True)\n",
    "\n",
    "\n",
    "#%% Line Processing\n",
    "def print_lines(parsed_list):\n",
    "    output = list()\n",
    "    for item in parsed_list:\n",
    "        pprint(item)\n",
    "        output.append(item)\n",
    "    return output\n",
    "\n",
    "\n",
    "def to_folder_dict(folder_list):\n",
    "    '''Combine folder info into dictionary.\n",
    "    '''\n",
    "    # TODO separate directory info from file info\n",
    "    #The first line in the folder list is the directory path\n",
    "    directory = ''\n",
    "    if folder_list:\n",
    "        d_list = folder_list[0]\n",
    "        if d_list:\n",
    "            directory = d_list[0]\n",
    "    folder_dict = {'Directory': directory}\n",
    "    for folder_info in folder_list[1:]:\n",
    "        filename, date, file_size = folder_info\n",
    "        full_path = '\\\\'.join([directory, filename])\n",
    "        file_parts = filename.rsplit('.', 1)\n",
    "        if len(file_parts) > 1:\n",
    "            extension = file_parts[1]\n",
    "        else:\n",
    "            extension = ''\n",
    "        folder_dict = {\n",
    "            'Path': full_path,\n",
    "            'Directory': directory,\n",
    "            'Filename': filename,\n",
    "            'Extension': extension,\n",
    "            'Date': date,\n",
    "            'Size': file_size\n",
    "            }\n",
    "    return folder_dict\n",
    "\n",
    "\n",
    "def make_files_table(dir_gen):\n",
    "    '''Combine folder info dictionaries into Pandas DataFrame.\n",
    "    '''\n",
    "    list_of_folders = list(dir_gen)\n",
    "    files_table = pd.DataFrame(list_of_folders)\n",
    "    files_table.set_index('Path')\n",
    "    return files_table\n",
    "\n",
    "\n",
    "#%% Reader definitions\n",
    "default_parser = tp.define_csv_parser('dir_files', delimiter=':',\n",
    "                                       skipinitialspace=True)\n",
    "heading_reader = ProcessingMethods([\n",
    "    default_parser,\n",
    "    tp.trim_items\n",
    "    ])\n",
    "folder_reader = ProcessingMethods([\n",
    "    RuleSet([skip_dir_rule, file_listing_rule, dir_header_rule,\n",
    "             skip_file_count_rule], default=default_parser),\n",
    "    tp.drop_blanks\n",
    "    ])\n",
    "summary_reader = ProcessingMethods([\n",
    "    RuleSet([file_count_rule, skip_totals_rule], default=default_parser),\n",
    "    tp.drop_blanks\n",
    "    ])\n",
    "\n",
    "\n",
    "#%% SectionBreak definitions\n",
    "folder_start = SectionBreak(\n",
    "    name='Start of Folder', sentinel='Directory of', break_offset='Before')\n",
    "folder_end = SectionBreak(name='End of Folder',sentinel=folder_summary_pt,\n",
    "                             break_offset='After')\n",
    "summary_start = SectionBreak(sentinel='Total Files Listed:',\n",
    "                                name='Start of DIR Summary', break_offset='Before')\n",
    "\n",
    "\n",
    "#%% Section definitions\n",
    "header_section = Section(\n",
    "    section_name='Header',\n",
    "    start_section=None,\n",
    "    end_section=folder_start,\n",
    "    processor=heading_reader,\n",
    "    aggregate=print_lines\n",
    "    )\n",
    "folder_section = Section(\n",
    "    section_name='Folder',\n",
    "    start_section=folder_start,\n",
    "    end_section=folder_end,\n",
    "    processor=folder_reader,\n",
    "    aggregate=to_folder_dict\n",
    "    )\n",
    "all_folder_section = Section(\n",
    "    section_name='All Folders',\n",
    "    start_section=folder_start,\n",
    "    end_section=summary_start,\n",
    "    processor=[folder_section],\n",
    "    aggregate=make_files_table\n",
    "    )\n",
    "summary_section = Section(\n",
    "    section_name='Summary',\n",
    "    start_section=summary_start,\n",
    "    end_section=None,\n",
    "    processor=summary_reader,\n",
    "    aggregate=tp.to_dict\n",
    "    )\n",
    "\n",
    "\n",
    "#%% Main Iteration\n",
    "def main():\n",
    "    # Test File\n",
    "    base_path = Path.cwd() / 'examples'\n",
    "    test_file = base_path / 'test_DIR_Data.txt'\n",
    "\n",
    "    # Call Primary routine\n",
    "    context = {\n",
    "        'File Name': test_file.name,\n",
    "        'File Path': test_file.parent,\n",
    "        'top_dir': str(base_path),\n",
    "        'tree_name': 'Test folder Tree'\n",
    "        }\n",
    "\n",
    "    source = tp.file_reader(test_file)\n",
    "    file_info = all_folder_section.read(source, context)\n",
    "    #summary = summary_section.read(source, **context)\n",
    "\n",
    "    # Output  Data\n",
    "    xw.view(file_info)\n",
    "    print('done')\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print('column index')\n",
    "print(''.join(str(i)*10 for i in range(10)))\n",
    "print(''.join(str(i) for i in range(10))*10)\n",
    "print(dir_text[9])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "a =dir_text[3]\n",
    "a.index('\\\\')\n",
    "a.rsplit('\\\\', 1)\n",
    "#'Folder Name:\\t' + a.rsplit('\\\\', 1)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fixed Width Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main part of a directory listing is formatted into columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column index\n",
      "0000000000111111111122222222223333333333444444444455555555556666666666\n",
      "0123456789012345678901234567890123456789012345678901234567890123456789\n",
      "2021-12-27  04:03 PM    <DIR>          Dir1\n",
      "2016-04-21  01:06 PM              3491 xcopy.txt\n"
     ]
    }
   ],
   "source": [
    "print('column index')\n",
    "print(''.join(str(i)*10 for i in range(7)))\n",
    "print(''.join(str(i) for i in range(10))*7)\n",
    "#print(''.join(divider))\n",
    "print(dir_text[7])\n",
    "print(dir_text[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "column_breaks=[11, 20, 29, 38]\n",
    "\n",
    "divider_list = ['.']*70\n",
    "for brk in column_breaks:\n",
    "    divider_list[brk] = '|'\n",
    "divider = ''.join(divider_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "column breaks\n",
      "...........|........|........|........|...............................\n",
      "2021-12-27  03:33 PM    <DIR>          ..\n",
      "2021-12-27  04:03 PM    <DIR>          Dir1\n",
      "2021-12-27  05:27 PM    <DIR>          Dir2\n",
      "2016-02-25  09:59 PM                 3 TestFile1.txt\n",
      "2016-02-15  06:46 PM                 7 TestFile2.rtf\n",
      "2016-02-15  06:47 PM                 0 TestFile3.docx\n",
      "2016-04-21  01:06 PM              3491 xcopy.txt\n",
      "...........|........|........|........|...............................\n"
     ]
    }
   ],
   "source": [
    "print('column breaks')\n",
    "#print(''.join(str(i)*10 for i in range(7)))\n",
    "#print(''.join(str(i) for i in range(10))*7)\n",
    "print(divider)\n",
    "\n",
    "for line in dir_text[6:13]:\n",
    "    print(line)\n",
    "    \n",
    "print(divider)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Part', ' 1'], ['Part', ' 2a'], ['Part', ' 2b']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = ['Part 1', 'Part 2a', 'Part 2b']\n",
    "b = tp.FixedWidthParser([4,3])\n",
    "[item for item in b.parser(a)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Part', ' 2a']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b.parse(a[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2016-04-21  01:06 PM', '          ', '    3491 ', 'xcopy.txt']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "a = tp.FixedWidthParser(locations=[20,30,39])\n",
    "a.parse(dir_text[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object FixedWidthParser.parser at 0x00000203D1DEE6D0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "b = tp.define_fixed_width_parser(locations=[20,30,39])\n",
    "b(dir_text[8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['2021-12-27  05:27 PM', '    <DIR> ', '         ', 'Dir2']]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "list(b(dir_text[8]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Functions\n",
    "def dir_name_split(dir_line):\n",
    "    output_dict = {'Folder Name': dir_line.rsplit('\\\\', 1)[1]}\n",
    "    return output_dict\n",
    "def file_count_split(dir_line):\n",
    "    output_dict = {'Number of Files': dir_line.strip().split(' ', 1)[0]}\n",
    "    return output_dict\n",
    "def get_subfolder_name(dir_line):\n",
    "    output_dict = {'Subdirectory': dir_line[36:]}\n",
    "    return output_dict\n",
    "def get_file_name(dir_line):\n",
    "    output_dict = {'File': dir_line[36:]}\n",
    "    return output_dict\n",
    "\n",
    "# Define Rules\n",
    "dir_name_rule = Rule('Directory of', pass_method=dir_name_split)\n",
    "subfolder_rule = Rule('<DIR>', pass_method=get_subfolder_name)\n",
    "file_count_rule = Rule('File(s)', pass_method=file_count_split)\n",
    "\n",
    "#Define Rule Set\n",
    "dir_process = RuleSet([dir_name_rule, subfolder_rule, file_count_rule], \n",
    "                      default=get_file_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t  Volume in drive C is Windows\n",
      "\t  Volume Serial Number is DAE7-D5BA\n",
      "\t \n",
      "\t  Directory of c:\\users\\...\\Test Dir Structure\n",
      "\t \n",
      "\t 2021-12-27  03:33 PM    <DIR>          .\n",
      "\t 2021-12-27  03:33 PM    <DIR>          ..\n",
      "\t 2021-12-27  04:03 PM    <DIR>          Dir1\n",
      "\t 2021-12-27  05:27 PM    <DIR>          Dir2\n",
      "\t 2016-02-25  09:59 PM                 3 TestFile1.txt\n",
      "\t 2016-02-15  06:46 PM                 7 TestFile2.rtf\n",
      "\t 2016-02-15  06:47 PM                 0 TestFile3.docx\n",
      "\t 2016-04-21  01:06 PM              3491 xcopy.txt\n",
      "\t                4 File(s)           3501 bytes\n",
      "\t \n",
      "\t  Directory of c:\\users\\...\\Test Dir Structure\\Dir1\n",
      "\t \n",
      "\t 2021-12-27  04:03 PM    <DIR>          .\n",
      "\t 2021-12-27  04:03 PM    <DIR>          ..\n",
      "\t 2016-02-15  06:48 PM                 0 File in Dir One.txt\n"
     ]
    }
   ],
   "source": [
    "for line in dir_text[0:20]:\n",
    "    print('\\t', line)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
